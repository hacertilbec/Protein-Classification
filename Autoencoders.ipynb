{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODERS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes are taken from Aurélien Géron's Hands-On Machine Learning with Scikit-Learn and TensorFlow book but I added some small changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 129 **2 # input is flatten version of 129x129 matrix\n",
    "n_hidden = 100 # hidden layer with 100 neurons\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = fully_connected(X, n_hidden, activation_fn=None)\n",
    "outputs = fully_connected(hidden, n_outputs, activation_fn=None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_sum(tf.square(outputs - X)) # MSE\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_iterations = 500 # Number of iterations\n",
    "codings = hidden # the output of the hidden layer provides the codings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearAutoencoder(X_train):\n",
    "    with tf.Session() as sess: \n",
    "        init.run()\n",
    "        for iteration in range(n_iterations):\n",
    "            training_op.run(feed_dict={X: X_train}) # no labels (unsupervised)\n",
    "        # Test on the same protein\n",
    "        codings_val = codings.eval(feed_dict={X: X_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedAutoencoder(X_train):\n",
    "    n_inputs = 129 * 129 # for pair distance matrix \n",
    "    n_hidden1 = 1000\n",
    "    n_hidden2 = 500 # codings \n",
    "    n_hidden3 = n_hidden1 \n",
    "    n_outputs = n_inputs\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.001\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs]) \n",
    "\n",
    "    with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected], activation_fn=tf.nn.elu, weights_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "        weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg)):\n",
    "\n",
    "        hidden1 = fully_connected(X, n_hidden1)\n",
    "        hidden2 = fully_connected(hidden1, n_hidden2) # codings\n",
    "        hidden3 = fully_connected(hidden2, n_hidden3)\n",
    "        outputs = fully_connected(hidden3, n_outputs, activation_fn=None)\n",
    "\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = reconstruction_loss + reg_losses\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    n_epochs = 5\n",
    "    batch_size = 150\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        init.run() \n",
    "        for epoch in range(n_epochs):\n",
    "            batch_start = 0 # starting index of the batch\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                X_batch = X_train[batch_start : batch_start+batch_size]\n",
    "                start += batch_size\n",
    "                sess.run(training_op, feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stacked Autoencoder with Tied Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is fairly straightforward, but there are a few important things to note:\n",
    "\n",
    "    • First,weight3andweights4arenotvariables,theyarerespectivelythetranspose of weights2 and weights1 (they are “tied” to them).\n",
    "\n",
    "    • Second, since they are not variables, it’s no use regularizing them: we only regula‐ rize weights1 and weights2.\n",
    "\n",
    "    • Third, biases are never tied, and never regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedAutoencoderWithTiedWeights(X_train):\n",
    "    n_inputs = 129 * 129 # for pair distance matrix \n",
    "    n_hidden1 = 1000\n",
    "    n_hidden2 = 500 # codings \n",
    "    n_hidden3 = n_hidden1 \n",
    "    n_outputs = n_inputs\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.001\n",
    "\n",
    "    activation = tf.nn.elu\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "    weights1_init = initializer([n_inputs, n_hidden1])\n",
    "    weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "\n",
    "    weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\") \n",
    "    weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\") \n",
    "    weights3 = tf.transpose(weights2, name=\"weights3\") # tied weights\n",
    "    weights4 = tf.transpose(weights1, name=\"weights4\") # tied weights\n",
    "\n",
    "    biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "    biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "    biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "    biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")\n",
    "\n",
    "    hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "    outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "    reg_loss = regularizer(weights1) + regularizer(weights2)\n",
    "\n",
    "    loss = reconstruction_loss + reg_loss\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        init.run() \n",
    "        for epoch in range(n_epochs):\n",
    "            batch_start = 0 # starting index of the batch\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                X_batch = X_train[batch_start : batch_start+batch_size]\n",
    "                start += batch_size\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
