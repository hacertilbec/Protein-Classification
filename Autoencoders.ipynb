{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODERS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes are taken from Aurélien Géron's Hands-On Machine Learning with Scikit-Learn and TensorFlow book but I added some small changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 129 **2 # input is flatten version of 129x129 matrix\n",
    "n_hidden = 100 # hidden layer with 100 neurons\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = fully_connected(X, n_hidden, activation_fn=None)\n",
    "outputs = fully_connected(hidden, n_outputs, activation_fn=None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_sum(tf.square(outputs - X)) # MSE\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_iterations = 500 # Number of iterations\n",
    "codings = hidden # the output of the hidden layer provides the codings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearAutoencoder(X_train):\n",
    "    with tf.Session() as sess: \n",
    "        init.run()\n",
    "        for iteration in range(n_iterations):\n",
    "            training_op.run(feed_dict={X: X_train}) # no labels (unsupervised)\n",
    "        # Test on the same protein\n",
    "        codings_val = codings.eval(feed_dict={X: X_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedAutoencoder(X_train):\n",
    "    n_inputs = 129 * 129 # for pair distance matrix \n",
    "    n_hidden1 = 1000\n",
    "    n_hidden2 = 500 # codings \n",
    "    n_hidden3 = n_hidden1 \n",
    "    n_outputs = n_inputs\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.001\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs]) \n",
    "\n",
    "    with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected], activation_fn=tf.nn.elu, weights_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "        weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg)):\n",
    "\n",
    "        hidden1 = fully_connected(X, n_hidden1)\n",
    "        hidden2 = fully_connected(hidden1, n_hidden2) # codings\n",
    "        hidden3 = fully_connected(hidden2, n_hidden3)\n",
    "        outputs = fully_connected(hidden3, n_outputs, activation_fn=None)\n",
    "\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = reconstruction_loss + reg_losses\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    n_epochs = 5\n",
    "    batch_size = 150\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        init.run() \n",
    "        for epoch in range(n_epochs):\n",
    "            batch_start = 0 # starting index of the batch\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                X_batch = X_train[batch_start : batch_start+batch_size]\n",
    "                start += batch_size\n",
    "                sess.run(training_op, feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stacked Autoencoder with Tied Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is fairly straightforward, but there are a few important things to note:\n",
    "\n",
    "    • First,weight3andweights4arenotvariables,theyarerespectivelythetranspose of weights2 and weights1 (they are “tied” to them).\n",
    "\n",
    "    • Second, since they are not variables, it’s no use regularizing them: we only regula‐ rize weights1 and weights2.\n",
    "\n",
    "    • Third, biases are never tied, and never regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedAutoencoderWithTiedWeights(X_train):\n",
    "    n_inputs = 129 * 129 # for pair distance matrix \n",
    "    n_hidden1 = 1000\n",
    "    n_hidden2 = 500 # codings \n",
    "    n_hidden3 = n_hidden1 \n",
    "    n_outputs = n_inputs\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.001\n",
    "\n",
    "    activation = tf.nn.elu\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "    weights1_init = initializer([n_inputs, n_hidden1])\n",
    "    weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "\n",
    "    weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\") \n",
    "    weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\") \n",
    "    weights3 = tf.transpose(weights2, name=\"weights3\") # tied weights\n",
    "    weights4 = tf.transpose(weights1, name=\"weights4\") # tied weights\n",
    "\n",
    "    biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "    biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "    biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "    biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")\n",
    "\n",
    "    hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "    outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "    reg_loss = regularizer(weights1) + regularizer(weights2)\n",
    "\n",
    "    loss = reconstruction_loss + reg_loss\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        init.run() \n",
    "        for epoch in range(n_epochs):\n",
    "            batch_start = 0 # starting index of the batch\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                X_batch = X_train[batch_start : batch_start+batch_size]\n",
    "                start += batch_size\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training one Autoencoder at a time in multiple graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to train one Autoencoder at a time. The first approach is to train each Autoencoder using a different graph, then we create the Stacked Autoencoder by simply initializing it with the weights and biases copied from these Autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def train_autoencoder(X_train, n_neurons, n_epochs,\n",
    "                      learning_rate = 0.01, l2_reg = 0.0005, seed=42,\n",
    "                      hidden_activation=tf.nn.elu,\n",
    "                      output_activation=tf.nn.elu):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, activation=hidden_activation, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=output_activation, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for protein in X_train:\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                sess.run(training_op, feed_dict={X: [protein]})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: [protein]})\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autoencoder(X_train):\n",
    "    init = tf.global_variables_initializer()\n",
    "    features = []\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for protein in X_train:\n",
    "            codings_val = hidden2.eval(feed_dict={X: [protein]})\n",
    "            features.append(codings_val[0])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
